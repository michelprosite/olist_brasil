{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório transient já existe.\n",
      "O diretório raw já existe.\n",
      "O diretório trusted já existe.\n",
      "O diretório refined já existe.\n",
      "O diretório engineer já existe.\n"
     ]
    }
   ],
   "source": [
    "def created_folders():\n",
    "    # Variavel responsável pos capiturar o path raiz do projeto\n",
    "    path_folder = os.getcwd() \n",
    "    # Abrindo e carregando o arquivo CSV com os nomes das pastas que serão criadas para o projeto\n",
    "    file_path = os.path.join(path_folder, 'nomes_diretorios.csv')\n",
    "    f = pd.read_csv(file_path)\n",
    "    lista_folders = f['Folders'].to_list()\n",
    "\n",
    "    #Criando o diretório \"data\"\n",
    "    if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "\n",
    "    # Laço for percorre a lista das partas cridas e inicia as pastas do projeto\n",
    "    for i in lista_folders:\n",
    "        diretorio = path_folder\n",
    "\n",
    "        # Verifica se as pastas já foram criadas, se não, as cria.\n",
    "        if not os.path.exists(diretorio + '/data/' + i):\n",
    "            os.makedirs(diretorio + '/data/' + i)\n",
    "            print(f\"Diretório {i} criado com sucesso!\")\n",
    "        else:\n",
    "            print(f\"O diretório {i} já existe.\")\n",
    "created_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 15:39:08,319 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /api/v1/datasets/download/olistbr/brazilian-ecommerce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos carregados e descompactados.\n"
     ]
    }
   ],
   "source": [
    "def download_csv_olist():\n",
    "    folder = os.getcwd()\n",
    "    path_folder = os.path.join(folder, 'data/transient')\n",
    "\n",
    "    files = os.listdir(path_folder)\n",
    "\n",
    "    if len(files) == 0:\n",
    "        PATH_FOLDER = path_folder\n",
    "        os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
    "\n",
    "        kaggle.api.dataset_download_files('olistbr/brazilian-ecommerce', path=PATH_FOLDER, unzip=True)\n",
    "        print('Arquivos carregados e descompactados.')\n",
    "    else:\n",
    "        print()\n",
    "        print(f'Arquivos já existem e ou foram atualizados!')\n",
    "        print('Caso queira atualiza-los se faz necessário executar a recarga da Raw para zerar os arquivos.')\n",
    "download_csv_olist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michel/Documentos/olist_brasil/data/controller.csv\n",
      "Arquivo controller criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "def created_arquivo_controller():\n",
    "    if os.path.exists(os.getcwd() + '/transient'):\n",
    "        folder = os.getcwd()\n",
    "        path_folder = os.path.join(folder, 'data/transient')  # Substitua pelo caminho da pasta desejada\n",
    "        file_path = os.path.join(folder, 'data/controller.csv')\n",
    "    else:\n",
    "        folder = os.getcwd()\n",
    "        path_folder = os.path.join(folder, 'data/raw')  # Substitua pelo caminho da pasta desejada\n",
    "        file_path = os.path.join(folder, 'data/controller.csv')\n",
    "\n",
    "    print(file_path)\n",
    "\n",
    "    # Capturar nomes dos arquivos e remover a extensão\n",
    "    arquivos = os.listdir(path_folder)\n",
    "\n",
    "    # Eliminar as palavras indesejadas\n",
    "    palavras_indesejadas = ['_dataset.csv', 'olist_', 'order_', '_dataset', 'product_', '_name', '_translation', '.csv', '.parquet']\n",
    "    nomes_limpos = [arquivo for arquivo in arquivos]\n",
    "    for palavra in palavras_indesejadas:\n",
    "        nomes_limpos = [nome.replace(palavra, '') for nome in nomes_limpos]\n",
    "\n",
    "    # Caminho do arquivo CSV a ser salvo\n",
    "    caminho_arquivo = str(path_folder[0]) + '/' + 'controller.csv'\n",
    "\n",
    "    # Definir os dados a serem escritos no arquivo CSV\n",
    "    dados = [\n",
    "        [\"path_transient\", \"path_raw\", \"path_trusted\", \"table_transient\", \"table_raw\", \"table_trusted\", \"table_name\", \"table_name_temp\"]\n",
    "    ] + [\n",
    "        [str(path_folder[0]) + \"data/transient/\", str(path_folder[0]) + \"data/raw/\", str(path_folder[0]) + \"data/trusted/\", nome.replace('.csv', \"\").replace('.parquet', \"\"), nome.replace('.csv', \"\").replace('.parquet', \"\"), nome.replace('.csv', \"\").replace('.parquet', \"\"), nome_limp, 'temp_' + nome_limp]\n",
    "        for nome, nome_limp in zip(arquivos, nomes_limpos)\n",
    "    ]\n",
    "\n",
    "    # Salvar o arquivo CSV\n",
    "    pd.DataFrame(dados).to_csv(file_path, index=False, header=False)\n",
    "    print('Arquivo controller criado com sucesso!')\n",
    "created_arquivo_controller()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_transient</th>\n",
       "      <th>path_raw</th>\n",
       "      <th>path_trusted</th>\n",
       "      <th>table_transient</th>\n",
       "      <th>table_raw</th>\n",
       "      <th>table_trusted</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_name_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/transient/</td>\n",
       "      <td>/data/raw/</td>\n",
       "      <td>/data/trusted/</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>olist_order_reviews_dataset</td>\n",
       "      <td>reviews</td>\n",
       "      <td>temp_reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/data/transient/</td>\n",
       "      <td>/data/raw/</td>\n",
       "      <td>/data/trusted/</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>olist_products_dataset</td>\n",
       "      <td>products</td>\n",
       "      <td>temp_products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/data/transient/</td>\n",
       "      <td>/data/raw/</td>\n",
       "      <td>/data/trusted/</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>olist_orders_dataset</td>\n",
       "      <td>orders</td>\n",
       "      <td>temp_orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/data/transient/</td>\n",
       "      <td>/data/raw/</td>\n",
       "      <td>/data/trusted/</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>product_category_name_translation</td>\n",
       "      <td>category</td>\n",
       "      <td>temp_category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/data/transient/</td>\n",
       "      <td>/data/raw/</td>\n",
       "      <td>/data/trusted/</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>olist_geolocation_dataset</td>\n",
       "      <td>geolocation</td>\n",
       "      <td>temp_geolocation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/data/transient/</td>\n",
       "      <td>/data/raw/</td>\n",
       "      <td>/data/trusted/</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>olist_order_items_dataset</td>\n",
       "      <td>items</td>\n",
       "      <td>temp_items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/data/transient/</td>\n",
       "      <td>/data/raw/</td>\n",
       "      <td>/data/trusted/</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>olist_customers_dataset</td>\n",
       "      <td>customers</td>\n",
       "      <td>temp_customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/data/transient/</td>\n",
       "      <td>/data/raw/</td>\n",
       "      <td>/data/trusted/</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>olist_sellers_dataset</td>\n",
       "      <td>sellers</td>\n",
       "      <td>temp_sellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/data/transient/</td>\n",
       "      <td>/data/raw/</td>\n",
       "      <td>/data/trusted/</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>olist_order_payments_dataset</td>\n",
       "      <td>payments</td>\n",
       "      <td>temp_payments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     path_transient    path_raw    path_trusted  \\\n",
       "0  /data/transient/  /data/raw/  /data/trusted/   \n",
       "1  /data/transient/  /data/raw/  /data/trusted/   \n",
       "2  /data/transient/  /data/raw/  /data/trusted/   \n",
       "3  /data/transient/  /data/raw/  /data/trusted/   \n",
       "4  /data/transient/  /data/raw/  /data/trusted/   \n",
       "5  /data/transient/  /data/raw/  /data/trusted/   \n",
       "6  /data/transient/  /data/raw/  /data/trusted/   \n",
       "7  /data/transient/  /data/raw/  /data/trusted/   \n",
       "8  /data/transient/  /data/raw/  /data/trusted/   \n",
       "\n",
       "                     table_transient                          table_raw  \\\n",
       "0        olist_order_reviews_dataset        olist_order_reviews_dataset   \n",
       "1             olist_products_dataset             olist_products_dataset   \n",
       "2               olist_orders_dataset               olist_orders_dataset   \n",
       "3  product_category_name_translation  product_category_name_translation   \n",
       "4          olist_geolocation_dataset          olist_geolocation_dataset   \n",
       "5          olist_order_items_dataset          olist_order_items_dataset   \n",
       "6            olist_customers_dataset            olist_customers_dataset   \n",
       "7              olist_sellers_dataset              olist_sellers_dataset   \n",
       "8       olist_order_payments_dataset       olist_order_payments_dataset   \n",
       "\n",
       "                       table_trusted   table_name   table_name_temp  \n",
       "0        olist_order_reviews_dataset      reviews      temp_reviews  \n",
       "1             olist_products_dataset     products     temp_products  \n",
       "2               olist_orders_dataset       orders       temp_orders  \n",
       "3  product_category_name_translation     category     temp_category  \n",
       "4          olist_geolocation_dataset  geolocation  temp_geolocation  \n",
       "5          olist_order_items_dataset        items        temp_items  \n",
       "6            olist_customers_dataset    customers    temp_customers  \n",
       "7              olist_sellers_dataset      sellers      temp_sellers  \n",
       "8       olist_order_payments_dataset     payments     temp_payments  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controller = os.path.join(os.getcwd(), 'data/controller.csv')\n",
    "path = pd.read_csv(controller)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path criado com base no arquivo controller!\n",
      "\n",
      "\n",
      "Arquivo /home/michel/Documentos/olist_brasil/data/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/data/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/data/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/data/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/data/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/data/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/data/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/data/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/data/controller.csv da Transient eliminado.\n"
     ]
    }
   ],
   "source": [
    "def carregando_raw():\n",
    "    folder = os.getcwd()\n",
    "    file_path = os.path.join(folder, 'data/controller.csv')\n",
    "    path = pd.read_csv(file_path)\n",
    "    print('Path criado com base no arquivo controller!')\n",
    "\n",
    "    for i in range(len(path['path_transient'])):\n",
    "        # Criando o caminho dos arquivos e diretórios\n",
    "        diretorio_transient = path['path_transient'][i]\n",
    "        nome_arquivo_transient = path['table_transient'][i] + '.csv'\n",
    "        caminho_arquivo_transient = (folder + diretorio_transient + nome_arquivo_transient)\n",
    "\n",
    "        diretorio_raw = path['path_raw'][i]\n",
    "        nome_arquivo_raw = path['table_raw'][i] + '.parquet'\n",
    "        caminho_arquivo_raw = (folder + diretorio_raw + nome_arquivo_raw)\n",
    "\n",
    "        # Verificando a existência dos arquivos em Transient e carrega na Raw pela primeira vez\n",
    "        if os.path.exists(caminho_arquivo_transient) and not os.path.exists(caminho_arquivo_raw):\n",
    "            # Carregando arquivos csv\n",
    "            df = pd.read_csv(caminho_arquivo_transient)\n",
    "\n",
    "            # Dropando os duplicados preservando o último atualizado\n",
    "            df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "\n",
    "            # Verifica se o DataFrame não está vazio\n",
    "            if not df.empty:\n",
    "                # Salva na Raw convertendo para o formato Parquet\n",
    "                df.to_parquet(caminho_arquivo_raw)\n",
    "                print(f\"Arquivo Parquet {nome_arquivo_raw} criado: {caminho_arquivo_raw}\")\n",
    "            else:\n",
    "                print(f\"O DataFrame está vazio: {nome_arquivo_raw}\")\n",
    "\n",
    "        # Caso a Raw já tenha sido carregada pela primeira vez, esse código é executado para atualizar os arquivos verificando se algum arquivo foi deletado.\n",
    "        elif os.path.exists(caminho_arquivo_transient) and os.path.exists(caminho_arquivo_raw):\n",
    "            # Confere a existência do arquivo, caso deletado, cria novamente\n",
    "            if not os.path.exists(caminho_arquivo_raw):\n",
    "                df = pd.read_csv(caminho_arquivo_transient)\n",
    "                df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "                if not df.empty:\n",
    "                    df.to_parquet(caminho_arquivo_raw)\n",
    "                    print(f\"Arquivo Parquet {nome_arquivo_raw} criado: {caminho_arquivo_raw}\")\n",
    "\n",
    "            # Carregando os DataFrames para concatenar os novos dados\n",
    "            df1 = pd.read_csv(caminho_arquivo_transient)\n",
    "            df2 = pd.read_parquet(caminho_arquivo_raw)\n",
    "\n",
    "            # Concatenando os dados\n",
    "            df_concatenado = pd.concat([df2, df1], axis=0)\n",
    "\n",
    "            # Eliminando as duplicatas pelo ID mantendo sempre a última ocorrência\n",
    "            df_concatenado = df_concatenado.drop_duplicates(subset=df_concatenado.columns[0], keep='last')\n",
    "            df_concatenado.to_parquet(caminho_arquivo_raw)\n",
    "        else:\n",
    "            print(f'Arquivo Raw {nome_arquivo_raw} já existe! Para atualizá-lo, carregue a transient correspondente.')\n",
    "\n",
    "    # Após a criação e atualização dos dados, os arquivos Transient são eliminados deixando o diretório\n",
    "    # livre para receber novos dados\n",
    "    print()\n",
    "    print()\n",
    "    for i in range(len(path['path_transient'])):\n",
    "        diretorio_transient = path['path_transient'][i]\n",
    "        nome_arquivo_transient = path['table_transient'][i] + '.csv'\n",
    "        caminho_arquivo_transient = (folder + diretorio_transient + nome_arquivo_transient)\n",
    "        \n",
    "        if os.path.exists(caminho_arquivo_transient):\n",
    "            os.remove(caminho_arquivo_transient)\n",
    "\n",
    "        print(f\"Arquivo {file_path} da Transient eliminado.\")\n",
    "\n",
    "carregando_raw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregando_trusted():\n",
    "    folder = os.getcwd()\n",
    "    file_path = os.path.join(folder, 'data/controller.csv')\n",
    "    path = pd.read_csv(file_path)\n",
    "\n",
    "    for i in range(len(path['path_raw'])):\n",
    "        # Criando o caminho dos arquivos e diretórios\n",
    "        diretorio_raw = path['path_raw'][i]\n",
    "        nome_arquivo_raw = path['table_raw'][i] + '.parquet'\n",
    "        caminho_arquivo_raw = (folder + diretorio_raw + nome_arquivo_raw)\n",
    "\n",
    "        diretorio_trusted = path['path_trusted'][i]\n",
    "        nome_arquivo_trusted = path['table_trusted'][i] + '.parquet'\n",
    "        caminho_arquivo_trusted = (folder + diretorio_trusted + nome_arquivo_trusted)\n",
    "\n",
    "        # Verificando a existência dos arquivos em Transient e carrega na Raw pela primeira vez\n",
    "        if os.path.exists(caminho_arquivo_raw) and not os.path.exists(caminho_arquivo_trusted):\n",
    "            # Carregando arquivos parquet\n",
    "            df = pd.read_parquet(caminho_arquivo_raw)\n",
    "\n",
    "            # Dropando os duplicados preservando o último atualizado\n",
    "            df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "\n",
    "            # Verifica se o DataFrame não está vazio\n",
    "            if not df.empty:\n",
    "                # Salva na Raw convertendo para o formato Parquet\n",
    "                df.to_parquet(caminho_arquivo_trusted)\n",
    "                print(f\"Arquivo Parquet {nome_arquivo_trusted} criado: {caminho_arquivo_trusted}\")\n",
    "            else:\n",
    "                print(f\"O DataFrame está vazio: {nome_arquivo_trusted}\")\n",
    "\n",
    "        # Caso a Raw já tenha sido carregada pela primeira vez, esse código é executado para atualizar os arquivos verificando se algum arquivo foi deletado.\n",
    "        elif os.path.exists(caminho_arquivo_raw) and os.path.exists(caminho_arquivo_trusted):\n",
    "            # Confere a existência do arquivo, caso deletado, cria novamente\n",
    "            if not os.path.exists(caminho_arquivo_trusted):\n",
    "                df = pd.read_parquet(caminho_arquivo_raw)\n",
    "                df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "                if not df.empty:\n",
    "                    df.to_parquet(caminho_arquivo_trusted)\n",
    "                    print(f\"Arquivo Parquet {nome_arquivo_trusted} criado: {caminho_arquivo_trusted}\")\n",
    "\n",
    "            # Carregando os DataFrames para concatenar os novos dados\n",
    "            df1 = pd.read_parquet(caminho_arquivo_raw)\n",
    "            df2 = pd.read_parquet(caminho_arquivo_trusted)\n",
    "\n",
    "            # Concatenando os dados\n",
    "            df_concatenado = pd.concat([df2, df1], axis=0)\n",
    "\n",
    "            # Eliminando as duplicatas pelo ID mantendo sempre a última ocorrência\n",
    "            df_concatenado = df_concatenado.drop_duplicates(subset=df_concatenado.columns[0], keep='last')\n",
    "            df_concatenado.to_parquet(caminho_arquivo_trusted)\n",
    "        else:\n",
    "            print(f'Arquivo Raw {nome_arquivo_trusted} já existe! Para atualizá-lo, carregue a transient correspondente.')\n",
    "carregando_trusted()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome da tabela: reviews\n",
      "caminho dos dados: /data/trusted/olist_order_reviews_dataset.parquet\n",
      "\n",
      "Nome da tabela: products\n",
      "caminho dos dados: /data/trusted/olist_products_dataset.parquet\n",
      "\n",
      "Nome da tabela: orders\n",
      "caminho dos dados: /data/trusted/olist_orders_dataset.parquet\n",
      "\n",
      "Nome da tabela: category\n",
      "caminho dos dados: /data/trusted/product_category_name_translation.parquet\n",
      "\n",
      "Nome da tabela: geolocation\n",
      "caminho dos dados: /data/trusted/olist_geolocation_dataset.parquet\n",
      "\n",
      "Nome da tabela: items\n",
      "caminho dos dados: /data/trusted/olist_order_items_dataset.parquet\n",
      "\n",
      "Nome da tabela: customers\n",
      "caminho dos dados: /data/trusted/olist_customers_dataset.parquet\n",
      "\n",
      "Nome da tabela: sellers\n",
      "caminho dos dados: /data/trusted/olist_sellers_dataset.parquet\n",
      "\n",
      "Nome da tabela: payments\n",
      "caminho dos dados: /data/trusted/olist_order_payments_dataset.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder = os.getcwd()\n",
    "file_path = os.path.join(folder, 'data/controller.csv')\n",
    "path = pd.read_csv(file_path)\n",
    "for i in range(len(path['table_name'])):\n",
    "    print(f\"Nome da tabela: {path['table_name'][i]}\")\n",
    "    print(f\"caminho dos dados: {path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = '*Santana25'\n",
    "\n",
    "for i in range(len(path['table_name_temp'])):\n",
    "    # Conectando ao servidor MySQL\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Criando um cursor para executar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se o banco 'olistdb' existe, se não, criá-lo\n",
    "    cursor.execute(\"SHOW DATABASES LIKE 'olistdb'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Conectar ao banco de dados 'olistdb'\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database='olistdb'\n",
    "    )\n",
    "\n",
    "    # Capturar os nomes das futuras tabelas em df['table_name_temp']\n",
    "    df = pd.read_parquet(os.getcwd() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet')\n",
    "    table_name_temp = path['table_name_temp'][i]\n",
    "\n",
    "    # Criar um novo cursor\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\n",
    "    cursor.execute(f\"SHOW TABLES LIKE '{table_name_temp}'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        # Obter as colunas e seus tipos a partir do arquivo Parquet\n",
    "        table_path = os.getcwd() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'\n",
    "        table_data = pd.read_parquet(table_path)\n",
    "        columns = table_data.columns\n",
    "        column_types = table_data.dtypes\n",
    "        table_index = table_data.index\n",
    "\n",
    "        # Criar a tabela com as colunas e tipos correspondentes\n",
    "        create_table_query = f\"CREATE TABLE {table_name_temp} (\"\n",
    "        for col, col_type in zip(columns, column_types):\n",
    "            if col_type == 'int64':\n",
    "                col_type = 'INT'\n",
    "            elif col_type == 'float64':\n",
    "                col_type = 'FLOAT'\n",
    "            elif col_type == 'bool':\n",
    "                col_type = 'BOOLEAN'\n",
    "            else:\n",
    "                col_type = 'VARCHAR(255)'  # Tipo padrão se não for numérico\n",
    "\n",
    "            if col == columns[0]:\n",
    "                create_table_query += f\"{col} {col_type} PRIMARY KEY, \"\n",
    "            else:\n",
    "                create_table_query += f\"{col} {col_type}, \"\n",
    "\n",
    "        create_table_query = create_table_query.rstrip(', ')  # Remover a última vírgula\n",
    "        create_table_query += \")\"\n",
    "        cursor.execute(create_table_query)\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Fechar a conexão com o banco de dados\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados na temp_reviews com sucesso!\n",
      "Dados carregados na temp_products com sucesso!\n",
      "Dados carregados na temp_orders com sucesso!\n",
      "Dados carregados na temp_category com sucesso!\n",
      "Dados carregados na temp_geolocation com sucesso!\n",
      "Dados carregados na temp_items com sucesso!\n",
      "Dados carregados na temp_customers com sucesso!\n",
      "Dados carregados na temp_sellers com sucesso!\n",
      "Dados carregados na temp_payments com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Santana25',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Função para limpar uma tabela\n",
    "def limpar_tabela(cursor, tabela):\n",
    "    cursor.execute(f\"DELETE FROM {tabela_temp}\")\n",
    "\n",
    "# Função para carregar dados de um arquivo Parquet para uma tabela\n",
    "def carregar_dados_parquet(cursor, tabela_temp, caminho):\n",
    "    dataframe = pd.read_parquet(caminho)\n",
    "    dataframe = dataframe.fillna(0)  # Substituir valores NaN por 0\n",
    "    colunas = dataframe.columns.tolist()\n",
    "    placeholders = ','.join(['%s'] * len(colunas))\n",
    "    \n",
    "    # Utilizar INSERT INTO ... VALUES (...) AS alias e substituir VALUES(col) pelo alias.col na cláusula ON DUPLICATE KEY UPDATE\n",
    "    insert_query = f\"INSERT INTO {tabela_temp} ({', '.join(colunas)}) VALUES ({placeholders}) AS tmp \" \\\n",
    "                  f\"ON DUPLICATE KEY UPDATE \" \\\n",
    "                  f\"{', '.join([f'{column}=tmp.{column}' for column in colunas])}\"\n",
    "    \n",
    "    valores = [tuple(row) for row in dataframe.values]\n",
    "    cursor.executemany(insert_query, valores)\n",
    "\n",
    "# Conectando ao banco de dados\n",
    "try:\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Verificando e carregando tabelas\n",
    "    for i in range(len(path['table_name_temp'])):\n",
    "        tabela_temp = path['table_name_temp'][i]\n",
    "        caminho = os.getcwdb().decode() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'\n",
    "        \n",
    "        # Limpar tabela antes de carregar os dados\n",
    "        limpar_tabela(cursor, tabela_temp)\n",
    "        \n",
    "        carregar_dados_parquet(cursor, tabela_temp, caminho)\n",
    "        cnx.commit()\n",
    "        print(f\"Dados carregados na {tabela_temp} com sucesso!\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if 'cursor' in locals():\n",
    "        cursor.close()\n",
    "    if 'cnx' in locals() and cnx.is_connected():\n",
    "        cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = '*Santana25'\n",
    "\n",
    "for i in range(len(path['table_name'])):\n",
    "    # Conectando ao servidor MySQL\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Criando um cursor para executar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se o banco 'olistdb' existe, se não, criá-lo\n",
    "    cursor.execute(\"SHOW DATABASES LIKE 'olistdb'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Conectar ao banco de dados 'olistdb'\n",
    "    connection = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database='olistdb'\n",
    "    )\n",
    "\n",
    "    # Capturar os nomes das futuras tabelas em df['table_name']\n",
    "    df = pd.read_parquet(os.getcwd() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet')\n",
    "    table_name = path['table_name'][i]\n",
    "\n",
    "    # Criar um novo cursor\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\n",
    "    cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
    "    result = cursor.fetchone()\n",
    "    if not result:\n",
    "        # Obter as colunas e seus tipos a partir do arquivo Parquet\n",
    "        table_path = os.getcwdb().decode() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'\n",
    "        table_data = pd.read_parquet(table_path)\n",
    "        columns = table_data.columns\n",
    "        column_types = table_data.dtypes\n",
    "        table_index = table_data.index\n",
    "\n",
    "        # Criar a tabela com as colunas e tipos correspondentes\n",
    "        create_table_query = f\"CREATE TABLE {table_name} (\"\n",
    "        for col, col_type in zip(columns, column_types):\n",
    "            if col_type == 'int64':\n",
    "                col_type = 'INT'\n",
    "            elif col_type == 'float64':\n",
    "                col_type = 'FLOAT'\n",
    "            elif col_type == 'bool':\n",
    "                col_type = 'BOOLEAN'\n",
    "            else:\n",
    "                col_type = 'VARCHAR(255)'  # Tipo padrão se não for numérico\n",
    "\n",
    "            if col == columns[0]:\n",
    "                create_table_query += f\"{col} {col_type} PRIMARY KEY, \"\n",
    "            else:\n",
    "                create_table_query += f\"{col} {col_type}, \"\n",
    "\n",
    "        create_table_query = create_table_query.rstrip(', ')  # Remover a última vírgula\n",
    "        create_table_query += \")\"\n",
    "        cursor.execute(create_table_query)\n",
    "\n",
    "    # Fechar o cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # Fechar a conexão com o banco de dados\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tabela reviews já possui dados e não será carregada.\n",
      "A tabela products já possui dados e não será carregada.\n",
      "A tabela orders já possui dados e não será carregada.\n",
      "A tabela category já possui dados e não será carregada.\n",
      "A tabela geolocation já possui dados e não será carregada.\n",
      "A tabela items já possui dados e não será carregada.\n",
      "A tabela customers já possui dados e não será carregada.\n",
      "A tabela sellers já possui dados e não será carregada.\n",
      "A tabela payments já possui dados e não será carregada.\n"
     ]
    }
   ],
   "source": [
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Santana25',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Função para verificar se uma tabela está vazia\n",
    "def tabela_vazia(cursor, tabela):\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {tabela}\")\n",
    "    result = cursor.fetchone()\n",
    "    return result[0] == 0\n",
    "\n",
    "# Função para carregar dados de um arquivo Parquet para uma tabela\n",
    "def carregar_dados_parquet(cursor, tabela, caminho):\n",
    "    dataframe = pd.read_parquet(caminho)\n",
    "    dataframe = dataframe.fillna(0)  # Substituir valores NaN por 0\n",
    "    valores = [tuple(row) for row in dataframe.values]\n",
    "    placeholders = ','.join(['%s'] * len(dataframe.columns))\n",
    "    cursor.executemany(f\"INSERT INTO {tabela} VALUES ({placeholders})\", valores)\n",
    "\n",
    "# Conectando ao banco de dados\n",
    "try:\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Verificando e carregando tabelas\n",
    "    for i in range(len(path['table_name'])):\n",
    "        tabela = path['table_name'][i]\n",
    "        if tabela_vazia(cursor, tabela):\n",
    "            caminho = os.getcwdb().decode() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'\n",
    "            carregar_dados_parquet(cursor, tabela, caminho)\n",
    "            cnx.commit()\n",
    "            print(f\"Dados carregados na tabela {tabela} com sucesso!\")\n",
    "        else:\n",
    "            print(f\"A tabela {tabela} já possui dados e não será carregada.\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if 'cursor' in locals():\n",
    "        cursor.close()\n",
    "    if 'cnx' in locals() and cnx.is_connected():\n",
    "        cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE executado com sucesso!\n",
      "INSERT executado com sucesso!\n",
      "DELETE executado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Configurações de conexão com o banco de dados MySQL\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': '*Santana25',\n",
    "    'host': 'localhost',\n",
    "    'database': 'olistdb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Código SQL\n",
    "sql_update = \"\"\"\n",
    "UPDATE `category` AS destino\n",
    "JOIN `temp_category` AS origem ON destino.`product_category_name` = origem.`product_category_name`\n",
    "SET destino.`product_category_name` = origem.`product_category_name`,\n",
    "    destino.`product_category_name` = origem.`product_category_name`\n",
    "\"\"\"\n",
    "\n",
    "sql_insert = \"\"\"\n",
    "INSERT INTO `category` (`product_category_name`)\n",
    "SELECT origem.`product_category_name`\n",
    "FROM `temp_category` AS origem\n",
    "LEFT JOIN `category` AS destino ON origem.`product_category_name` = destino.`product_category_name`\n",
    "WHERE destino.`product_category_name` IS NULL\n",
    "\"\"\"\n",
    "\n",
    "sql_delete = \"\"\"\n",
    "DELETE destino\n",
    "FROM `category` AS destino\n",
    "LEFT JOIN `temp_category` AS origem ON destino.`product_category_name` = origem.`product_category_name`\n",
    "WHERE origem.`product_category_name` IS NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Conectando ao banco de dados\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # Executando as consultas SQL\n",
    "    cursor.execute(sql_update)\n",
    "    print(\"UPDATE executado com sucesso!\")\n",
    "\n",
    "    cursor.execute(sql_insert)\n",
    "    print(\"INSERT executado com sucesso!\")\n",
    "\n",
    "    cursor.execute(sql_delete)\n",
    "    print(\"DELETE executado com sucesso!\")\n",
    "\n",
    "    cnx.commit()\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if cnx.is_connected():\n",
    "        cnx.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************************\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ontem 15:05] Rodrigo Aparecido Kartcheski\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "primeiro passo\n",
    "\n",
    "* achar as primary key\n",
    "\n",
    " \n",
    "\n",
    "segundo passo\n",
    "\n",
    "* achar quais tabelas se relacionam - buscar relacionametno entre fatos e dimensões - fazer o desenho da modelagem\n",
    "\n",
    " \n",
    "\n",
    "terceiro passo\n",
    "\n",
    "* fazer join das tabelas\n",
    "\n",
    " \n",
    "\n",
    "quarto passo\n",
    "\n",
    "* fazer a carga dos dados usando (CRUD)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
