{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório engineer já existe.\n",
      "O diretório raw já existe.\n",
      "O diretório refined já existe.\n",
      "O diretório transient já existe.\n",
      "O diretório trusted já existe.\n"
     ]
    }
   ],
   "source": [
    "def created_folders():\n",
    "    # Variavel responsável pos capiturar o path raiz do projeto\n",
    "    path_folder = os.getcwd() \n",
    "    # Abrindo e carregando o arquivo CSV com os nomes das pastas que serão criadas para o projeto\n",
    "    file_path = os.path.join(path_folder, 'nome_diretorios.csv')\n",
    "    f = pd.read_csv(file_path)\n",
    "    lista_folders = f['Folders'].to_list()\n",
    "\n",
    "    #Criando o diretório \"data\"\n",
    "    if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "\n",
    "    # Laço for percorre a lista das partas cridas e inicia as pastas do projeto\n",
    "    for i in lista_folders:\n",
    "        diretorio = path_folder\n",
    "\n",
    "        # Verifica se as pastas já foram criadas, se não, as cria.\n",
    "        if not os.path.exists(diretorio + '/data/' + i):\n",
    "            os.makedirs(diretorio + '/data/' + i)\n",
    "            print(f\"Diretório {i} criado com sucesso!\")\n",
    "        else:\n",
    "            print(f\"O diretório {i} já existe.\")\n",
    "created_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos carregados e descompactados.\n"
     ]
    }
   ],
   "source": [
    "def download_csv_olist():\n",
    "    folder = os.getcwd()\n",
    "    path_folder = os.path.join(folder, 'data/transient')\n",
    "\n",
    "    files = os.listdir(path_folder)\n",
    "\n",
    "    if len(files) == 0:\n",
    "        PATH_FOLDER = path_folder\n",
    "        os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
    "\n",
    "        kaggle.api.dataset_download_files('olistbr/brazilian-ecommerce', path=PATH_FOLDER, unzip=True)\n",
    "        print('Arquivos carregados e descompactados.')\n",
    "    else:\n",
    "        print()\n",
    "        print(f'Arquivos já existem e ou foram atualizados!')\n",
    "        print('Caso queira atualiza-los se faz necessário executar a recarga da Raw para zerar os arquivos.')\n",
    "download_csv_olist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo controller criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "def created_arquivo_controller():\n",
    "    folder = os.getcwd()\n",
    "    path_folder = os.path.join(folder, 'data/transient')  # Substitua pelo caminho da pasta desejada\n",
    "    file_path = os.path.join(folder, 'controller.csv')\n",
    "\n",
    "    # Capturar nomes dos arquivos e remover a extensão\n",
    "    arquivos = os.listdir(path_folder)\n",
    "\n",
    "    # Eliminar as palavras indesejadas\n",
    "    palavras_indesejadas = ['_dataset.csv', 'olist_', 'order_', '_dataset', 'product_', '_name', '_translation', '.csv']\n",
    "    nomes_limpos = [arquivo for arquivo in arquivos]\n",
    "    for palavra in palavras_indesejadas:\n",
    "        nomes_limpos = [nome.replace(palavra, '') for nome in nomes_limpos]\n",
    "\n",
    "    # Caminho do arquivo CSV a ser salvo\n",
    "    caminho_arquivo = str(path_folder[0]) + '/' + 'controller.csv'\n",
    "\n",
    "    # Definir os dados a serem escritos no arquivo CSV\n",
    "    dados = [\n",
    "        [\"path_transient\", \"path_raw\", \"path_trusted\", \"table_transient\", \"table_raw\", \"table_trusted\", \"table_name\", \"table_name_temp\"]\n",
    "    ] + [\n",
    "        [str(path_folder[0]) + \"data/transient/\", str(path_folder[0]) + \"data/raw/\", str(path_folder[0]) + \"data/trusted/\", nome.replace('.csv', \"\"), nome.replace('.csv', \"\"), nome.replace('.csv', \"\"), nome_limp, 'temp_' + nome_limp]\n",
    "        for nome, nome_limp in zip(arquivos, nomes_limpos)\n",
    "    ]\n",
    "\n",
    "    # Salvar o arquivo CSV\n",
    "    pd.DataFrame(dados).to_csv(file_path, index=False, header=False)\n",
    "    print('Arquivo controller criado com sucesso!')\n",
    "created_arquivo_controller()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path criado com base no arquivo controller!\n",
      "\n",
      "\n",
      "Arquivo /home/michel/Documentos/olist_brasil/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/controller.csv da Transient eliminado.\n",
      "Arquivo /home/michel/Documentos/olist_brasil/controller.csv da Transient eliminado.\n"
     ]
    }
   ],
   "source": [
    "def carregando_raw():\n",
    "    folder = os.getcwd()\n",
    "    file_path = os.path.join(folder, 'controller.csv')\n",
    "    path = pd.read_csv(file_path)\n",
    "    print('Path criado com base no arquivo controller!')\n",
    "\n",
    "    for i in range(len(path['path_transient'])):\n",
    "        # Criando o caminho dos arquivos e diretórios\n",
    "        diretorio_transient = path['path_transient'][i]\n",
    "        nome_arquivo_transient = path['table_transient'][i] + '.csv'\n",
    "        caminho_arquivo_transient = (folder + diretorio_transient + nome_arquivo_transient)\n",
    "\n",
    "        diretorio_raw = path['path_raw'][i]\n",
    "        nome_arquivo_raw = path['table_raw'][i] + '.parquet'\n",
    "        caminho_arquivo_raw = (folder + diretorio_raw + nome_arquivo_raw)\n",
    "\n",
    "        # Verificando a existência dos arquivos em Transient e carrega na Raw pela primeira vez\n",
    "        if os.path.exists(caminho_arquivo_transient) and not os.path.exists(caminho_arquivo_raw):\n",
    "            # Carregando arquivos csv\n",
    "            df = pd.read_csv(caminho_arquivo_transient)\n",
    "\n",
    "            # Dropando os duplicados preservando o último atualizado\n",
    "            df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "\n",
    "            # Verifica se o DataFrame não está vazio\n",
    "            if not df.empty:\n",
    "                # Salva na Raw convertendo para o formato Parquet\n",
    "                df.to_parquet(caminho_arquivo_raw)\n",
    "                print(f\"Arquivo Parquet {nome_arquivo_raw} criado: {caminho_arquivo_raw}\")\n",
    "            else:\n",
    "                print(f\"O DataFrame está vazio: {nome_arquivo_raw}\")\n",
    "\n",
    "        # Caso a Raw já tenha sido carregada pela primeira vez, esse código é executado para atualizar os arquivos verificando se algum arquivo foi deletado.\n",
    "        elif os.path.exists(caminho_arquivo_transient) and os.path.exists(caminho_arquivo_raw):\n",
    "            # Confere a existência do arquivo, caso deletado, cria novamente\n",
    "            if not os.path.exists(caminho_arquivo_raw):\n",
    "                df = pd.read_csv(caminho_arquivo_transient)\n",
    "                df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "                if not df.empty:\n",
    "                    df.to_parquet(caminho_arquivo_raw)\n",
    "                    print(f\"Arquivo Parquet {nome_arquivo_raw} criado: {caminho_arquivo_raw}\")\n",
    "\n",
    "            # Carregando os DataFrames para concatenar os novos dados\n",
    "            df1 = pd.read_csv(caminho_arquivo_transient)\n",
    "            df2 = pd.read_parquet(caminho_arquivo_raw)\n",
    "\n",
    "            # Concatenando os dados\n",
    "            df_concatenado = pd.concat([df2, df1], axis=0)\n",
    "\n",
    "            # Eliminando as duplicatas pelo ID mantendo sempre a última ocorrência\n",
    "            df_concatenado = df_concatenado.drop_duplicates(subset=df_concatenado.columns[0], keep='last')\n",
    "            df_concatenado.to_parquet(caminho_arquivo_raw)\n",
    "        else:\n",
    "            print(f'Arquivo Raw {nome_arquivo_raw} já existe! Para atualizá-lo, carregue a transient correspondente.')\n",
    "\n",
    "    # Após a criação e atualização dos dados, os arquivos Transient são eliminados deixando o diretório\n",
    "    # livre para receber novos dados\n",
    "    print()\n",
    "    print()\n",
    "    for i in range(len(path['path_transient'])):\n",
    "        diretorio_transient = path['path_transient'][i]\n",
    "        nome_arquivo_transient = path['table_transient'][i] + '.csv'\n",
    "        caminho_arquivo_transient = (folder + diretorio_transient + nome_arquivo_transient)\n",
    "        \n",
    "        if os.path.exists(caminho_arquivo_transient):\n",
    "            os.remove(caminho_arquivo_transient)\n",
    "\n",
    "        print(f\"Arquivo {file_path} da Transient eliminado.\")\n",
    "\n",
    "carregando_raw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregando_trusted():\n",
    "    folder = os.getcwd()\n",
    "    file_path = os.path.join(folder, 'controller.csv')\n",
    "    path = pd.read_csv(file_path)\n",
    "\n",
    "    for i in range(len(path['path_raw'])):\n",
    "        # Criando o caminho dos arquivos e diretórios\n",
    "        diretorio_raw = path['path_raw'][i]\n",
    "        nome_arquivo_raw = path['table_raw'][i] + '.parquet'\n",
    "        caminho_arquivo_raw = (folder + diretorio_raw + nome_arquivo_raw)\n",
    "\n",
    "        diretorio_trusted = path['path_trusted'][i]\n",
    "        nome_arquivo_trusted = path['table_trusted'][i] + '.parquet'\n",
    "        caminho_arquivo_trusted = (folder + diretorio_trusted + nome_arquivo_trusted)\n",
    "\n",
    "        # Verificando a existência dos arquivos em Transient e carrega na Raw pela primeira vez\n",
    "        if os.path.exists(caminho_arquivo_raw) and not os.path.exists(caminho_arquivo_trusted):\n",
    "            # Carregando arquivos parquet\n",
    "            df = pd.read_parquet(caminho_arquivo_raw)\n",
    "\n",
    "            # Dropando os duplicados preservando o último atualizado\n",
    "            df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "\n",
    "            # Verifica se o DataFrame não está vazio\n",
    "            if not df.empty:\n",
    "                # Salva na Raw convertendo para o formato Parquet\n",
    "                df.to_parquet(caminho_arquivo_trusted)\n",
    "                print(f\"Arquivo Parquet {nome_arquivo_trusted} criado: {caminho_arquivo_trusted}\")\n",
    "            else:\n",
    "                print(f\"O DataFrame está vazio: {nome_arquivo_trusted}\")\n",
    "\n",
    "        # Caso a Raw já tenha sido carregada pela primeira vez, esse código é executado para atualizar os arquivos verificando se algum arquivo foi deletado.\n",
    "        elif os.path.exists(caminho_arquivo_raw) and os.path.exists(caminho_arquivo_trusted):\n",
    "            # Confere a existência do arquivo, caso deletado, cria novamente\n",
    "            if not os.path.exists(caminho_arquivo_trusted):\n",
    "                df = pd.read_parquet(caminho_arquivo_raw)\n",
    "                df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
    "                if not df.empty:\n",
    "                    df.to_parquet(caminho_arquivo_trusted)\n",
    "                    print(f\"Arquivo Parquet {nome_arquivo_trusted} criado: {caminho_arquivo_trusted}\")\n",
    "\n",
    "            # Carregando os DataFrames para concatenar os novos dados\n",
    "            df1 = pd.read_parquet(caminho_arquivo_raw)\n",
    "            df2 = pd.read_parquet(caminho_arquivo_trusted)\n",
    "\n",
    "            # Concatenando os dados\n",
    "            df_concatenado = pd.concat([df2, df1], axis=0)\n",
    "\n",
    "            # Eliminando as duplicatas pelo ID mantendo sempre a última ocorrência\n",
    "            df_concatenado = df_concatenado.drop_duplicates(subset=df_concatenado.columns[0], keep='last')\n",
    "            df_concatenado.to_parquet(caminho_arquivo_trusted)\n",
    "        else:\n",
    "            print(f'Arquivo Raw {nome_arquivo_trusted} já existe! Para atualizá-lo, carregue a transient correspondente.')\n",
    "carregando_trusted()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
