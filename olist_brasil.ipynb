{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelprosite/olist_brasil/blob/main/olist_brasil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "bZ6Srf7VyDyT",
        "outputId": "d6a4b3de-bbf2-438c-af59-12ab7e13b8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HC9eipQWx57m",
        "outputId": "41001cab-f751-4c6b-e48b-674069872ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bd604fa498c4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkaggle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mApiClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\u001b[0m in \u001b[0;36mauthenticate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mconfig_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_config_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 raise IOError('Could not find {}. Make sure it\\'s located in'\n\u001b[0m\u001b[1;32m    165\u001b[0m                               ' {}. Or use the environment method.'.format(\n\u001b[1;32m    166\u001b[0m                                   self.config_file, self.config_dir))\n",
            "\u001b[0;31mOSError\u001b[0m: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pyb3Kh7x57o"
      },
      "outputs": [],
      "source": [
        "def created_folders():\n",
        "    # Variavel responsável pos capiturar o path raiz do projeto\n",
        "    path_folder = os.getcwd() \n",
        "    # Abrindo e carregando o arquivo CSV com os nomes das pastas que serão criadas para o projeto\n",
        "    file_path = os.path.join(path_folder, 'nomes_diretorios.csv')\n",
        "    f = pd.read_csv(file_path)\n",
        "    lista_folders = f['Folders'].to_list()\n",
        "\n",
        "    #Criando o diretório \"data\"\n",
        "    if not os.path.exists('data'):\n",
        "            os.makedirs('data')\n",
        "\n",
        "    # Laço for percorre a lista das partas cridas e inicia as pastas do projeto\n",
        "    for i in lista_folders:\n",
        "        diretorio = path_folder\n",
        "\n",
        "        # Verifica se as pastas já foram criadas, se não, as cria.\n",
        "        if not os.path.exists(diretorio + '/data/' + i):\n",
        "            os.makedirs(diretorio + '/data/' + i)\n",
        "            print(f\"Diretório {i} criado com sucesso!\")\n",
        "        else:\n",
        "            print(f\"O diretório {i} já existe.\")\n",
        "created_folders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmMCkLOmx57p"
      },
      "outputs": [],
      "source": [
        "def download_csv_olist():\n",
        "    folder = os.getcwd()\n",
        "    path_folder = os.path.join(folder, 'data/transient')\n",
        "\n",
        "    files = os.listdir(path_folder)\n",
        "\n",
        "    if len(files) == 0:\n",
        "        PATH_FOLDER = path_folder\n",
        "        os.environ['PATH_FOLDER'] = PATH_FOLDER\n",
        "\n",
        "        kaggle.api.dataset_download_files('olistbr/brazilian-ecommerce', path=PATH_FOLDER, unzip=True)\n",
        "        print('Arquivos carregados e descompactados.')\n",
        "    else:\n",
        "        print()\n",
        "        print(f'Arquivos já existem e ou foram atualizados!')\n",
        "        print('Caso queira atualiza-los se faz necessário executar a recarga da Raw para zerar os arquivos.')\n",
        "download_csv_olist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y13kIFuWx57q"
      },
      "outputs": [],
      "source": [
        "def created_arquivo_controller():\n",
        "    if os.path.exists(os.getcwd() + '/transient'):\n",
        "        folder = os.getcwd()\n",
        "        path_folder = os.path.join(folder, 'data/transient')  # Substitua pelo caminho da pasta desejada\n",
        "        file_path = os.path.join(folder, 'data/controller.csv')\n",
        "    else:\n",
        "        folder = os.getcwd()\n",
        "        path_folder = os.path.join(folder, 'data/raw')  # Substitua pelo caminho da pasta desejada\n",
        "        file_path = os.path.join(folder, 'data/controller.csv')\n",
        "\n",
        "    print(file_path)\n",
        "\n",
        "    # Capturar nomes dos arquivos e remover a extensão\n",
        "    arquivos = os.listdir(path_folder)\n",
        "\n",
        "    # Eliminar as palavras indesejadas\n",
        "    palavras_indesejadas = ['_dataset.csv', 'olist_', 'order_', '_dataset', 'product_', '_name', '_translation', '.csv', '.parquet']\n",
        "    nomes_limpos = [arquivo for arquivo in arquivos]\n",
        "    for palavra in palavras_indesejadas:\n",
        "        nomes_limpos = [nome.replace(palavra, '') for nome in nomes_limpos]\n",
        "\n",
        "    # Caminho do arquivo CSV a ser salvo\n",
        "    caminho_arquivo = str(path_folder[0]) + '/' + 'controller.csv'\n",
        "\n",
        "    # Definir os dados a serem escritos no arquivo CSV\n",
        "    dados = [\n",
        "        [\"path_transient\", \"path_raw\", \"path_trusted\", \"table_transient\", \"table_raw\", \"table_trusted\", \"table_name\", \"table_name_temp\"]\n",
        "    ] + [\n",
        "        [str(path_folder[0]) + \"data/transient/\", str(path_folder[0]) + \"data/raw/\", str(path_folder[0]) + \"data/trusted/\", nome.replace('.csv', \"\").replace('.parquet', \"\"), nome.replace('.csv', \"\").replace('.parquet', \"\"), nome.replace('.csv', \"\").replace('.parquet', \"\"), nome_limp, 'temp_' + nome_limp]\n",
        "        for nome, nome_limp in zip(arquivos, nomes_limpos)\n",
        "    ]\n",
        "\n",
        "    # Salvar o arquivo CSV\n",
        "    pd.DataFrame(dados).to_csv(file_path, index=False, header=False)\n",
        "    print('Arquivo controller criado com sucesso!')\n",
        "created_arquivo_controller()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfbP6dlWx57r"
      },
      "outputs": [],
      "source": [
        "controller = os.path.join(os.getcwd(), 'data/controller.csv')\n",
        "path = pd.read_csv(controller)\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd9AM8mox57r"
      },
      "outputs": [],
      "source": [
        "def carregando_raw():\n",
        "    folder = os.getcwd()\n",
        "    file_path = os.path.join(folder, 'data/controller.csv')\n",
        "    path = pd.read_csv(file_path)\n",
        "    print('Path criado com base no arquivo controller!')\n",
        "\n",
        "    for i in range(len(path['path_transient'])):\n",
        "        # Criando o caminho dos arquivos e diretórios\n",
        "        diretorio_transient = path['path_transient'][i]\n",
        "        nome_arquivo_transient = path['table_transient'][i] + '.csv'\n",
        "        caminho_arquivo_transient = (folder + diretorio_transient + nome_arquivo_transient)\n",
        "\n",
        "        diretorio_raw = path['path_raw'][i]\n",
        "        nome_arquivo_raw = path['table_raw'][i] + '.parquet'\n",
        "        caminho_arquivo_raw = (folder + diretorio_raw + nome_arquivo_raw)\n",
        "\n",
        "        # Verificando a existência dos arquivos em Transient e carrega na Raw pela primeira vez\n",
        "        if os.path.exists(caminho_arquivo_transient) and not os.path.exists(caminho_arquivo_raw):\n",
        "            # Carregando arquivos csv\n",
        "            df = pd.read_csv(caminho_arquivo_transient)\n",
        "\n",
        "            # Dropando os duplicados preservando o último atualizado\n",
        "            df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
        "\n",
        "            # Verifica se o DataFrame não está vazio\n",
        "            if not df.empty:\n",
        "                # Salva na Raw convertendo para o formato Parquet\n",
        "                df.to_parquet(caminho_arquivo_raw)\n",
        "                print(f\"Arquivo Parquet {nome_arquivo_raw} criado: {caminho_arquivo_raw}\")\n",
        "            else:\n",
        "                print(f\"O DataFrame está vazio: {nome_arquivo_raw}\")\n",
        "\n",
        "        # Caso a Raw já tenha sido carregada pela primeira vez, esse código é executado para atualizar os arquivos verificando se algum arquivo foi deletado.\n",
        "        elif os.path.exists(caminho_arquivo_transient) and os.path.exists(caminho_arquivo_raw):\n",
        "            # Confere a existência do arquivo, caso deletado, cria novamente\n",
        "            if not os.path.exists(caminho_arquivo_raw):\n",
        "                df = pd.read_csv(caminho_arquivo_transient)\n",
        "                df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
        "                if not df.empty:\n",
        "                    df.to_parquet(caminho_arquivo_raw)\n",
        "                    print(f\"Arquivo Parquet {nome_arquivo_raw} criado: {caminho_arquivo_raw}\")\n",
        "\n",
        "            # Carregando os DataFrames para concatenar os novos dados\n",
        "            df1 = pd.read_csv(caminho_arquivo_transient)\n",
        "            df2 = pd.read_parquet(caminho_arquivo_raw)\n",
        "\n",
        "            # Concatenando os dados\n",
        "            df_concatenado = pd.concat([df2, df1], axis=0)\n",
        "\n",
        "            # Eliminando as duplicatas pelo ID mantendo sempre a última ocorrência\n",
        "            df_concatenado = df_concatenado.drop_duplicates(subset=df_concatenado.columns[0], keep='last')\n",
        "            df_concatenado.to_parquet(caminho_arquivo_raw)\n",
        "        else:\n",
        "            print(f'Arquivo Raw {nome_arquivo_raw} já existe! Para atualizá-lo, carregue a transient correspondente.')\n",
        "\n",
        "    # Após a criação e atualização dos dados, os arquivos Transient são eliminados deixando o diretório\n",
        "    # livre para receber novos dados\n",
        "    print()\n",
        "    print()\n",
        "    for i in range(len(path['path_transient'])):\n",
        "        diretorio_transient = path['path_transient'][i]\n",
        "        nome_arquivo_transient = path['table_transient'][i] + '.csv'\n",
        "        caminho_arquivo_transient = (folder + diretorio_transient + nome_arquivo_transient)\n",
        "        \n",
        "        if os.path.exists(caminho_arquivo_transient):\n",
        "            os.remove(caminho_arquivo_transient)\n",
        "\n",
        "        print(f\"Arquivo {file_path} da Transient eliminado.\")\n",
        "\n",
        "carregando_raw()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZGqnjY4x57s"
      },
      "outputs": [],
      "source": [
        "def carregando_trusted():\n",
        "    folder = os.getcwd()\n",
        "    file_path = os.path.join(folder, 'data/controller.csv')\n",
        "    path = pd.read_csv(file_path)\n",
        "\n",
        "    for i in range(len(path['path_raw'])):\n",
        "        # Criando o caminho dos arquivos e diretórios\n",
        "        diretorio_raw = path['path_raw'][i]\n",
        "        nome_arquivo_raw = path['table_raw'][i] + '.parquet'\n",
        "        caminho_arquivo_raw = (folder + diretorio_raw + nome_arquivo_raw)\n",
        "\n",
        "        diretorio_trusted = path['path_trusted'][i]\n",
        "        nome_arquivo_trusted = path['table_trusted'][i] + '.parquet'\n",
        "        caminho_arquivo_trusted = (folder + diretorio_trusted + nome_arquivo_trusted)\n",
        "\n",
        "        # Verificando a existência dos arquivos em Transient e carrega na Raw pela primeira vez\n",
        "        if os.path.exists(caminho_arquivo_raw) and not os.path.exists(caminho_arquivo_trusted):\n",
        "            # Carregando arquivos parquet\n",
        "            df = pd.read_parquet(caminho_arquivo_raw)\n",
        "\n",
        "            # Dropando os duplicados preservando o último atualizado\n",
        "            df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
        "\n",
        "            # Verifica se o DataFrame não está vazio\n",
        "            if not df.empty:\n",
        "                # Salva na Raw convertendo para o formato Parquet\n",
        "                df.to_parquet(caminho_arquivo_trusted)\n",
        "                print(f\"Arquivo Parquet {nome_arquivo_trusted} criado: {caminho_arquivo_trusted}\")\n",
        "            else:\n",
        "                print(f\"O DataFrame está vazio: {nome_arquivo_trusted}\")\n",
        "\n",
        "        # Caso a Raw já tenha sido carregada pela primeira vez, esse código é executado para atualizar os arquivos verificando se algum arquivo foi deletado.\n",
        "        elif os.path.exists(caminho_arquivo_raw) and os.path.exists(caminho_arquivo_trusted):\n",
        "            # Confere a existência do arquivo, caso deletado, cria novamente\n",
        "            if not os.path.exists(caminho_arquivo_trusted):\n",
        "                df = pd.read_parquet(caminho_arquivo_raw)\n",
        "                df.drop_duplicates(subset=df.columns[0], keep='last', inplace=True)\n",
        "                if not df.empty:\n",
        "                    df.to_parquet(caminho_arquivo_trusted)\n",
        "                    print(f\"Arquivo Parquet {nome_arquivo_trusted} criado: {caminho_arquivo_trusted}\")\n",
        "\n",
        "            # Carregando os DataFrames para concatenar os novos dados\n",
        "            df1 = pd.read_parquet(caminho_arquivo_raw)\n",
        "            df2 = pd.read_parquet(caminho_arquivo_trusted)\n",
        "\n",
        "            # Concatenando os dados\n",
        "            df_concatenado = pd.concat([df2, df1], axis=0)\n",
        "\n",
        "            # Eliminando as duplicatas pelo ID mantendo sempre a última ocorrência\n",
        "            df_concatenado = df_concatenado.drop_duplicates(subset=df_concatenado.columns[0], keep='last')\n",
        "            df_concatenado.to_parquet(caminho_arquivo_trusted)\n",
        "        else:\n",
        "            print(f'Arquivo Raw {nome_arquivo_trusted} já existe! Para atualizá-lo, carregue a transient correspondente.')\n",
        "carregando_trusted()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sftHSRSx57t"
      },
      "outputs": [],
      "source": [
        "folder = os.getcwd()\n",
        "file_path = os.path.join(folder, 'data/controller.csv')\n",
        "path = pd.read_csv(file_path)\n",
        "for i in range(len(path['table_name'])):\n",
        "    print(f\"Nome da tabela: {path['table_name'][i]}\")\n",
        "    print(f\"caminho dos dados: {path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnwrH7sgx57t"
      },
      "outputs": [],
      "source": [
        "import mysql.connector\n",
        "\n",
        "# Configurações de conexão com o banco de dados\n",
        "host = 'localhost'\n",
        "user = 'root'\n",
        "password = '*Santana25'\n",
        "\n",
        "for i in range(len(path['table_name_temp'])):\n",
        "    # Conectando ao servidor MySQL\n",
        "    connection = mysql.connector.connect(\n",
        "        host=host,\n",
        "        user=user,\n",
        "        password=password\n",
        "    )\n",
        "\n",
        "    # Criando um cursor para executar comandos SQL\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Verificar se o banco 'olistdb' existe, se não, criá-lo\n",
        "    cursor.execute(\"SHOW DATABASES LIKE 'olistdb'\")\n",
        "    result = cursor.fetchone()\n",
        "    if not result:\n",
        "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
        "\n",
        "    # Fechar o cursor\n",
        "    cursor.close()\n",
        "\n",
        "    # Conectar ao banco de dados 'olistdb'\n",
        "    connection = mysql.connector.connect(\n",
        "        host=host,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        database='olistdb'\n",
        "    )\n",
        "\n",
        "    # Capturar os nomes das futuras tabelas em df['table_name_temp']\n",
        "    df = pd.read_parquet(os.getcwd() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet')\n",
        "    table_name_temp = path['table_name_temp'][i]\n",
        "\n",
        "    # Criar um novo cursor\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\n",
        "    cursor.execute(f\"SHOW TABLES LIKE '{table_name_temp}'\")\n",
        "    result = cursor.fetchone()\n",
        "    if not result:\n",
        "        # Obter as colunas e seus tipos a partir do arquivo Parquet\n",
        "        table_path = os.getcwd() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'\n",
        "        table_data = pd.read_parquet(table_path)\n",
        "        columns = table_data.columns\n",
        "        column_types = table_data.dtypes\n",
        "        table_index = table_data.index\n",
        "\n",
        "        # Criar a tabela com as colunas e tipos correspondentes\n",
        "        create_table_query = f\"CREATE TABLE {table_name_temp} (\"\n",
        "        for col, col_type in zip(columns, column_types):\n",
        "            if col_type == 'int64':\n",
        "                col_type = 'INT'\n",
        "            elif col_type == 'float64':\n",
        "                col_type = 'FLOAT'\n",
        "            elif col_type == 'bool':\n",
        "                col_type = 'BOOLEAN'\n",
        "            else:\n",
        "                col_type = 'VARCHAR(255)'  # Tipo padrão se não for numérico\n",
        "\n",
        "            if col == columns[0]:\n",
        "                create_table_query += f\"{col} {col_type} PRIMARY KEY, \"\n",
        "            else:\n",
        "                create_table_query += f\"{col} {col_type}, \"\n",
        "\n",
        "        create_table_query = create_table_query.rstrip(', ')  # Remover a última vírgula\n",
        "        create_table_query += \")\"\n",
        "        cursor.execute(create_table_query)\n",
        "\n",
        "    # Fechar o cursor\n",
        "    cursor.close()\n",
        "\n",
        "    # Fechar a conexão com o banco de dados\n",
        "    connection.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T9nTUNix57t"
      },
      "outputs": [],
      "source": [
        "# Configurações de conexão com o banco de dados MySQL\n",
        "config = {\n",
        "    'user': 'root',\n",
        "    'password': '*Santana25',\n",
        "    'host': 'localhost',\n",
        "    'database': 'olistdb',\n",
        "    'raise_on_warnings': True\n",
        "}\n",
        "\n",
        "# Função para limpar uma tabela\n",
        "def limpar_tabela(cursor, tabela):\n",
        "    cursor.execute(f\"DELETE FROM {tabela_temp}\")\n",
        "\n",
        "# Função para carregar dados de um arquivo Parquet para uma tabela\n",
        "def carregar_dados_parquet(cursor, tabela_temp, caminho):\n",
        "    dataframe = pd.read_parquet(caminho)\n",
        "    dataframe = dataframe.fillna(0)  # Substituir valores NaN por 0\n",
        "    colunas = dataframe.columns.tolist()\n",
        "    placeholders = ','.join(['%s'] * len(colunas))\n",
        "    \n",
        "    # Utilizar INSERT INTO ... VALUES (...) AS alias e substituir VALUES(col) pelo alias.col na cláusula ON DUPLICATE KEY UPDATE\n",
        "    insert_query = f\"INSERT INTO {tabela_temp} ({', '.join(colunas)}) VALUES ({placeholders}) AS tmp \" \\\n",
        "                  f\"ON DUPLICATE KEY UPDATE \" \\\n",
        "                  f\"{', '.join([f'{column}=tmp.{column}' for column in colunas])}\"\n",
        "    \n",
        "    valores = [tuple(row) for row in dataframe.values]\n",
        "    cursor.executemany(insert_query, valores)\n",
        "\n",
        "# Conectando ao banco de dados\n",
        "try:\n",
        "    cnx = mysql.connector.connect(**config)\n",
        "    cursor = cnx.cursor()\n",
        "\n",
        "    # Verificando e carregando tabelas\n",
        "    for i in range(len(path['table_name_temp'])):\n",
        "        tabela_temp = path['table_name_temp'][i]\n",
        "        caminho = os.getcwdb().decode() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'\n",
        "        \n",
        "        # Limpar tabela antes de carregar os dados\n",
        "        limpar_tabela(cursor, tabela_temp)\n",
        "        \n",
        "        carregar_dados_parquet(cursor, tabela_temp, caminho)\n",
        "        cnx.commit()\n",
        "        print(f\"Dados carregados na {tabela_temp} com sucesso!\")\n",
        "\n",
        "except mysql.connector.Error as err:\n",
        "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
        "\n",
        "finally:\n",
        "    if 'cursor' in locals():\n",
        "        cursor.close()\n",
        "    if 'cnx' in locals() and cnx.is_connected():\n",
        "        cnx.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut85QJ0Ix57u"
      },
      "outputs": [],
      "source": [
        "import mysql.connector\n",
        "\n",
        "# Configurações de conexão com o banco de dados\n",
        "host = 'localhost'\n",
        "user = 'root'\n",
        "password = '*Santana25'\n",
        "\n",
        "for i in range(len(path['table_name'])):\n",
        "    # Conectando ao servidor MySQL\n",
        "    connection = mysql.connector.connect(\n",
        "        host=host,\n",
        "        user=user,\n",
        "        password=password\n",
        "    )\n",
        "\n",
        "    # Criando um cursor para executar comandos SQL\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Verificar se o banco 'olistdb' existe, se não, criá-lo\n",
        "    cursor.execute(\"SHOW DATABASES LIKE 'olistdb'\")\n",
        "    result = cursor.fetchone()\n",
        "    if not result:\n",
        "        cursor.execute(\"CREATE DATABASE olistdb\")\n",
        "\n",
        "    # Fechar o cursor\n",
        "    cursor.close()\n",
        "\n",
        "    # Conectar ao banco de dados 'olistdb'\n",
        "    connection = mysql.connector.connect(\n",
        "        host=host,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        database='olistdb'\n",
        "    )\n",
        "\n",
        "    # Capturar os nomes das futuras tabelas em df['table_name']\n",
        "    df = pd.read_parquet(os.getcwd() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet')\n",
        "    table_name = path['table_name'][i]\n",
        "\n",
        "    # Criar um novo cursor\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Verificar se as tabelas existem, se não, criar as tabelas no banco de dados\n",
        "    cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
        "    result = cursor.fetchone()\n",
        "    if not result:\n",
        "        # Obter as colunas e seus tipos a partir do arquivo Parquet\n",
        "        table_path = os.getcwdb().decode() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'\n",
        "        table_data = pd.read_parquet(table_path)\n",
        "        columns = table_data.columns\n",
        "        column_types = table_data.dtypes\n",
        "        table_index = table_data.index\n",
        "\n",
        "        # Criar a tabela com as colunas e tipos correspondentes\n",
        "        create_table_query = f\"CREATE TABLE {table_name} (\"\n",
        "        for col, col_type in zip(columns, column_types):\n",
        "            if col_type == 'int64':\n",
        "                col_type = 'INT'\n",
        "            elif col_type == 'float64':\n",
        "                col_type = 'FLOAT'\n",
        "            elif col_type == 'bool':\n",
        "                col_type = 'BOOLEAN'\n",
        "            else:\n",
        "                col_type = 'VARCHAR(255)'  # Tipo padrão se não for numérico\n",
        "\n",
        "            if col == columns[0]:\n",
        "                create_table_query += f\"{col} {col_type} PRIMARY KEY, \"\n",
        "            else:\n",
        "                create_table_query += f\"{col} {col_type}, \"\n",
        "\n",
        "        create_table_query = create_table_query.rstrip(', ')  # Remover a última vírgula\n",
        "        create_table_query += \")\"\n",
        "        cursor.execute(create_table_query)\n",
        "\n",
        "    # Fechar o cursor\n",
        "    cursor.close()\n",
        "\n",
        "    # Fechar a conexão com o banco de dados\n",
        "    connection.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyEESCnhx57u"
      },
      "outputs": [],
      "source": [
        "# Configurações de conexão com o banco de dados MySQL\n",
        "config = {\n",
        "    'user': 'root',\n",
        "    'password': '*Santana25',\n",
        "    'host': 'localhost',\n",
        "    'database': 'olistdb',\n",
        "    'raise_on_warnings': True\n",
        "}\n",
        "\n",
        "# Função para verificar se uma tabela está vazia\n",
        "def tabela_vazia(cursor, tabela):\n",
        "    cursor.execute(f\"SELECT COUNT(*) FROM {tabela}\")\n",
        "    result = cursor.fetchone()\n",
        "    return result[0] == 0\n",
        "\n",
        "# Função para carregar dados de um arquivo Parquet para uma tabela\n",
        "def carregar_dados_parquet(cursor, tabela, caminho):\n",
        "    dataframe = pd.read_parquet(caminho)\n",
        "    dataframe = dataframe.fillna(0)  # Substituir valores NaN por 0\n",
        "    valores = [tuple(row) for row in dataframe.values]\n",
        "    placeholders = ','.join(['%s'] * len(dataframe.columns))\n",
        "    cursor.executemany(f\"INSERT INTO {tabela} VALUES ({placeholders})\", valores)\n",
        "\n",
        "# Conectando ao banco de dados\n",
        "try:\n",
        "    cnx = mysql.connector.connect(**config)\n",
        "    cursor = cnx.cursor()\n",
        "\n",
        "    # Verificando e carregando tabelas\n",
        "    for i in range(len(path['table_name'])):\n",
        "        tabela = path['table_name'][i]\n",
        "        if tabela_vazia(cursor, tabela):\n",
        "            caminho = os.getcwdb().decode() + path['path_trusted'][i] + path['table_trusted'][i] + '.parquet'\n",
        "            carregar_dados_parquet(cursor, tabela, caminho)\n",
        "            cnx.commit()\n",
        "            print(f\"Dados carregados na tabela {tabela} com sucesso!\")\n",
        "        else:\n",
        "            print(f\"A tabela {tabela} já possui dados e não será carregada.\")\n",
        "\n",
        "except mysql.connector.Error as err:\n",
        "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
        "\n",
        "finally:\n",
        "    if 'cursor' in locals():\n",
        "        cursor.close()\n",
        "    if 'cnx' in locals() and cnx.is_connected():\n",
        "        cnx.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4Tdp4X_x57v"
      },
      "outputs": [],
      "source": [
        "# Configurações de conexão com o banco de dados MySQL\n",
        "config = {\n",
        "    'user': 'root',\n",
        "    'password': '*Santana25',\n",
        "    'host': 'localhost',\n",
        "    'database': 'olistdb',\n",
        "    'raise_on_warnings': True\n",
        "}\n",
        "\n",
        "# Código SQL\n",
        "sql_update = \"\"\"\n",
        "UPDATE `category` AS destino\n",
        "JOIN `temp_category` AS origem ON destino.`product_category_name` = origem.`product_category_name`\n",
        "SET destino.`product_category_name` = origem.`product_category_name`,\n",
        "    destino.`product_category_name` = origem.`product_category_name`\n",
        "\"\"\"\n",
        "\n",
        "sql_insert = \"\"\"\n",
        "INSERT INTO `category` (`product_category_name`)\n",
        "SELECT origem.`product_category_name`\n",
        "FROM `temp_category` AS origem\n",
        "LEFT JOIN `category` AS destino ON origem.`product_category_name` = destino.`product_category_name`\n",
        "WHERE destino.`product_category_name` IS NULL\n",
        "\"\"\"\n",
        "\n",
        "sql_delete = \"\"\"\n",
        "DELETE destino\n",
        "FROM `category` AS destino\n",
        "LEFT JOIN `temp_category` AS origem ON destino.`product_category_name` = origem.`product_category_name`\n",
        "WHERE origem.`product_category_name` IS NULL\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    # Conectando ao banco de dados\n",
        "    cnx = mysql.connector.connect(**config)\n",
        "    cursor = cnx.cursor()\n",
        "\n",
        "    # Executando as consultas SQL\n",
        "    cursor.execute(sql_update)\n",
        "    print(\"UPDATE executado com sucesso!\")\n",
        "\n",
        "    cursor.execute(sql_insert)\n",
        "    print(\"INSERT executado com sucesso!\")\n",
        "\n",
        "    cursor.execute(sql_delete)\n",
        "    print(\"DELETE executado com sucesso!\")\n",
        "\n",
        "    cnx.commit()\n",
        "\n",
        "except mysql.connector.Error as err:\n",
        "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
        "\n",
        "finally:\n",
        "    if cursor:\n",
        "        cursor.close()\n",
        "    if cnx.is_connected():\n",
        "        cnx.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FurffSPyx57v"
      },
      "source": [
        "****************************\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5qdtdVMx57w"
      },
      "source": [
        "[Ontem 15:05] Rodrigo Aparecido Kartcheski\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "primeiro passo\n",
        "\n",
        "* achar as primary key\n",
        "\n",
        " \n",
        "\n",
        "segundo passo\n",
        "\n",
        "* achar quais tabelas se relacionam - buscar relacionametno entre fatos e dimensões - fazer o desenho da modelagem\n",
        "\n",
        " \n",
        "\n",
        "terceiro passo\n",
        "\n",
        "* fazer join das tabelas\n",
        "\n",
        " \n",
        "\n",
        "quarto passo\n",
        "\n",
        "* fazer a carga dos dados usando (CRUD)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFmPGKJJx57x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}